import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture

np.random.seed(0)
x1 = np.random.normal(-2, 1, (300, 2))
x2 = np.random.normal(2, 1, (300, 2))
x = np.vstack((x1, x2))

gmm = GaussianMixture(n_components=2)
gmm.fit(x)
labels = gmm.predict(x)

plt.scatter(x[:, 0], x[:, 1], c=labels)
plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='x')
plt.title('GMM Clustering') 
plt.xlabel('Feature 1') 
plt.ylabel('Feature 2') 
plt.show()



from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

iris = load_iris()
X, y = iris.data, iris.target 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy of the NaÃ¯ve Bayesian classifier: {accuracy * 100:.2f}%')

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
print("Confusion Matrix:")

print(confusion_matrix(y_test, y_pred))
new_samples = [[5.0, 3.5, 1.5, 0.2], [6.5, 3.0, 5.5, 2.0]]
predictions = model.predict(new_samples)
predicted_species = iris.target_names[predictions]
print('Predictions for new samples:', predicted_species)








import pandas as pd
from pgmpy.models import DiscreteBayesianNetwork as BN
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

# Load and preprocess
df = pd.read_csv("heart (1).csv")
df["target"] = (df["target"] > 0).astype(int)
df = df[["age", "sex", "cp", "chol", "thalach", "target"]]
df['age'] = pd.cut(df['age'], [28,40,55,77], labels=[0,1,2]).astype(int)
df['chol'] = pd.cut(df['chol'], [120,200,300,600], labels=[0,1,2]).astype(int)
df['thalach'] = pd.cut(df['thalach'], [70,120,160,210], labels=[0,1,2]).astype(int)

# Build and train model
model = BN([('age','target'),('sex','target'),('cp','target'),('chol','target'),('thalach','target')])
model.fit(df, estimator=MaximumLikelihoodEstimator)

# Inference
infer = VariableElimination(model)
res = infer.query(variables=['target'], evidence={'age':1,'sex':1,'cp':3,'chol':1,'thalach':2})

# Output
print("\nProbability of Heart Disease:\n", res)







import numpy as np

x=np.array([[2,9],[1,5],[3,6]],dtype=float)
y=np.array([[92],[86],[89]],dtype=float)
x=x/np.amax(x,axis=0)
y=y/100

np.random.seed(0)
wh=np.random.uniform(size=(2,3))
bh=np.random.uniform(size=(1,3))
wout=np.random.uniform(size=(3,1))
bout=np.random.uniform(size=(1,1))

for _ in range(5000):
    hidden=1/(1+np.exp(-(x@wh+bh)))
    output=1/(1+np.exp(-(hidden@wout+bout)))
    output_error=y-output
    output_delta=output_error*output*(1-output)
    hidden_error=output_delta @wout.T
    hidden_delta=hidden_error*hidden*(1-hidden)
    wout+=hidden.T @output_delta*0.1
    wh+=x.T @hidden_delta*0.1


print("Input: \n" + str(x))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n", output)









import numpy as np,random

n=3
Q=np.zeros((n,n,4))
def step(s,a):
    d=[(-1,0),(0,1),(1,0),(0,-1)]
    s=tuple(np.clip(np.add(s,d[a]),0,n-1))
    return s,99*(s==(n-1,n-1))-1

for ep in range(500):
    s=(0,0);eps=max(.1,1*.99**ep)
    while s!=(n-1,n-1):
        a=random.randrange(4) if random.random()<eps else Q[s].argmax()
        ns,r=step(s,a)
        Q[s][a]+=0.1*(r+0.9*Q[ns].max()-Q[s][a])
        s=ns

print(Q)
print(Q.argmax(2))

s=(0,0)
while s!=(n-1,n-1):
    s,_=step(s,Q[s].argmax())
    print(s)
